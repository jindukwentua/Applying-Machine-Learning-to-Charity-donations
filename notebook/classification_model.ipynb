{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Importing python libraries\r\n",
    "#\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy.stats import mode\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import seaborn as sns; sns.set(style = 'darkgrid')\r\n",
    "import requests\r\n",
    "from io import StringIO\r\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\r\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "from catboost import CatBoostClassifier\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import export_graphviz \r\n",
    "from IPython.display import Image  \r\n",
    "import pydotplus\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(action = 'ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = pd.read_csv('../notebook/cleaned_train_data.csv')\r\n",
    "test_data = pd.read_csv('../notebook/cleaned_test_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.drop(\"Unnamed: 0\", axis=1, inplace=True)\r\n",
    "test_data.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data.shape, test_data.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Selecting the independent variables and the target variable\r\n",
    "#\r\n",
    "y = train_data['donated']\r\n",
    "X = train_data.drop('donated', axis = 1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **Joining the Train and Test data to encode the categorical columns together**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total_data = pd.concat([X, test_data])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categorical_columns = []\r\n",
    "for col in total_data.columns:\r\n",
    "    unique_column =total_data[col].nunique()\r\n",
    "    if unique_column <=60:\r\n",
    "        categorical_columns.append(col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(categorical_columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "le=LabelEncoder()\r\n",
    "for i in list(categorical_columns):\r\n",
    "        total_data[i]=le.fit_transform(total_data[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **Splitting the Train and Test data to after encoding the categorical columns**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train_cat_encoded =  total_data.iloc[:19372]\r\n",
    "X_test_cat_encoded =  total_data.iloc[19372:]\r\n",
    "X_train_cat_encoded.shape, X_test_cat_encoded.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Splitting the data into training and testing sets\r\n",
    "#\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_cat_encoded, y, test_size = 0.3, random_state = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# define and configure the model\r\n",
    "model = KNeighborsClassifier()\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "# Measuring the accuracy of the model\r\n",
    "#\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# define and configure the model\r\n",
    "model = DecisionTreeClassifier()\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\r\n",
    "# Creating a dataframe of features and their respective importances\r\n",
    "#\r\n",
    "impo_df = pd.DataFrame({'feature': X_train_cat_encoded.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a bar chart of feature importance in descending order\r\n",
    "#\r\n",
    "impo_df = impo_df[:10].sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\r\n",
    "plt.legend(loc = 'center right')\r\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\r\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# define and configure the model\r\n",
    "model = XGBClassifier(random_state=43)\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\r\n",
    "# Creating a dataframe of features and their respective importances\r\n",
    "#\r\n",
    "impo_df = pd.DataFrame({'feature': X_train_cat_encoded.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a bar chart of feature importance in descending order\r\n",
    "#\r\n",
    "impo_df = impo_df[:10].sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\r\n",
    "plt.legend(loc = 'center right')\r\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\r\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define and configure the model\r\n",
    "model = CatBoostClassifier(verbose=False)\r\n",
    "\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\r\n",
    "# Creating a dataframe of features and their respective importances\r\n",
    "#\r\n",
    "impo_df = pd.DataFrame({'feature': X_train_cat_encoded.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a bar chart of feature importance in descending order\r\n",
    "#\r\n",
    "impo_df = impo_df[:10].sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\r\n",
    "plt.legend(loc = 'center right')\r\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\r\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# define and configure the model\r\n",
    "model = LogisticRegression()\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define and configure the model\r\n",
    "model = RandomForestClassifier(random_state = 0)\r\n",
    "\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\r\n",
    "# Creating a dataframe of features and their respective importances\r\n",
    "#\r\n",
    "impo_df = pd.DataFrame({'feature': X_train_cat_encoded.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a bar chart of feature importance in descending order\r\n",
    "#\r\n",
    "impo_df = impo_df[:10].sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\r\n",
    "plt.legend(loc = 'center right')\r\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\r\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define and configure the model\r\n",
    "model = AdaBoostClassifier(random_state = 0)\r\n",
    "\r\n",
    "# fit the model\r\n",
    "model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# evaluate the model\r\n",
    "preds = model.predict(X_test)\r\n",
    "print('Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, preds)} and the f1 score is {f1_score(y_test, preds)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, preds)}')\r\n",
    "confusion_matrix(y_test, preds)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\r\n",
    "# Creating a dataframe of features and their respective importances\r\n",
    "#\r\n",
    "impo_df = pd.DataFrame({'feature': X_train_cat_encoded.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a bar chart of feature importance in descending order\r\n",
    "#\r\n",
    "impo_df = impo_df[:10].sort_values(by = 'importance', ascending = False)\r\n",
    "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\r\n",
    "plt.legend(loc = 'center right')\r\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\r\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Modelling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Modelling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RandomForest Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Previewing the parameters to tune\r\n",
    "#\r\n",
    "RandomForestClassifier()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a dictionary of parameters to tune\r\n",
    "#\r\n",
    "params = {'n_estimators': [10, 20, 30, 50, 100],\r\n",
    "         'max_depth': [1, 2, 3, 4, 5]}\r\n",
    "\r\n",
    "# Setting the number of folds to 10 and instantiating the model\r\n",
    "# \r\n",
    "kfold = KFold(n_splits=10, shuffle=True)\r\n",
    "model = RandomForestClassifier()\r\n",
    "\r\n",
    "search = GridSearchCV(model, param_grid=params, scoring = 'f1', cv = kfold)\r\n",
    "\r\n",
    "# Fitting the grid search with the X and the y variables\r\n",
    "#\r\n",
    "search.fit(X_train_cat_encoded, y)\r\n",
    "\r\n",
    "# Checking for the best parameters\r\n",
    "#\r\n",
    "print(f'The best parameters are: {search.best_params_}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Applying the best parameters to the model\r\n",
    "\r\n",
    "\r\n",
    "# Splitting the data into training and testing sets\r\n",
    "#\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_cat_encoded, y, test_size = 0.3, random_state = 0)\r\n",
    "\r\n",
    "# Instantiating the model\r\n",
    "#\r\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state = 0)\r\n",
    "rf.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Making predictions\r\n",
    "#\r\n",
    "y_pred = rf.predict(X_test)\r\n",
    "\r\n",
    "# Measuring the accuracy of the model\r\n",
    "#\r\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, y_pred)} and the f1 score is {f1_score(y_test, y_pred)}')\r\n",
    "print('\\n')\r\n",
    "print(f'{classification_report(y_test, y_pred)}')\r\n",
    "confusion_matrix(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Parameter tuning hasn't decreased or increased the f1 score*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross Validation to check for the stability of the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Performing cross validation of ten folds\r\n",
    "#\r\n",
    "scores = cross_val_score(rf, X_train_cat_encoded, y, scoring = 'f1', cv = 10)\r\n",
    "\r\n",
    "# Calculating the mean of the cross validation scores\r\n",
    "#\r\n",
    "print(f'Mean of cross validation scores is {scores.mean()}')\r\n",
    "\r\n",
    "# Calculating the variance of the cross validation scores from the mean\r\n",
    "#\r\n",
    "print(f'Standard deviation of the cross validation scores is {scores.std()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenging the solution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Using different models to check whether performance can be improved*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y__ =  y.copy()\r\n",
    "total_test_set_prediction = []\r\n",
    "kf = KFold(n_splits=5, random_state=False, shuffle=False)\r\n",
    "for train_index, test_index in kf.split(X_train_cat_encoded):\r\n",
    "    X_train, X_test = X_train_cat_encoded.iloc[train_index], X_train_cat_encoded.iloc[test_index]\r\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\r\n",
    "    rf_model = RandomForestClassifier(n_estimators=600,random_state=0)\r\n",
    "\r\n",
    "    # fit the model\r\n",
    "    rf_model.fit(X_train, y_train)\r\n",
    "    # evaluate the model\r\n",
    "    preds = rf_model.predict(X_test)\r\n",
    "    test_preds  = rf_model.predict(X_test_cat_encoded)\r\n",
    "    total_test_set_prediction.append(test_preds)\r\n",
    "    #arr = np.stack([arr,test_preds],1)\r\n",
    "    y__.iloc[test_index] = preds\r\n",
    "    print('k-fold Accuracy : ',accuracy_score(y_test, preds))\r\n",
    "print('Cross_validation_Accuracy : ',accuracy_score(y__, y))\r\n",
    "test_pred_array = np.array(total_test_set_prediction)\r\n",
    "test_pred_array_T = test_pred_array.transpose()\r\n",
    "final_test_pred  = mode(test_pred_array_T,1)[0]\r\n",
    "final_test_pred = final_test_pred.reshape((-1))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted_donors = pd.DataFrame()\r\n",
    "predicted_donors['donated'] = final_test_pred\r\n",
    "predicted_donors['control_no'] = test_control_no\r\n",
    "predicted_donors.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finding the model with the highest accuracy\r\n",
    "#\r\n",
    "scores.loc['mean'].idxmax()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*An alternative model that can match or outperform the AdaBoostClassifier is the XGBClassifier. More data is needed to increase the predictive power of the model. As the data is highky imbalanced, The f1 score metric of success has been used*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* *Out of all the models used to predict whether a person has hypothyroid, the AdaBoostClassifier performs well with an f1 score of approximately 86%*\r\n",
    "\r\n",
    "\r\n",
    "*  *The best performing kernel in the SupportVectorClassifier is Linear with an accuracy score 98.5% of and an f1 score of 86.%*\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8eb07b4d783c51f574b3398e25d662e9253b789236e8fa3b76d1d4089bc7b5f6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}